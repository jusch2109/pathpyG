{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "import pathpyG as pp\n",
    "pp.config['torch']['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_outdegrees(graph):\n",
    "    weighted_outdegree = scatter(graph.data.edge_weight, graph.data.edge_index[0], dim=0, dim_size=graph.data.num_nodes, reduce='sum')\n",
    "    return weighted_outdegree\n",
    "\n",
    "def compute_transition_probabilities(graph):\n",
    "    weighted_outdegree = compute_weighted_outdegrees(graph)\n",
    "    source_ids = graph.data.edge_index[0]\n",
    "    return graph.data.edge_weight/ weighted_outdegree[source_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Pairing_function\n",
    "# https://math.stackexchange.com/questions/1377929/generalization-of-cantor-pairing-function-to-triples-and-n-tuples\n",
    "def cantor_pairing(x, y):\n",
    "    \"\"\"\n",
    "    Computes the Cantor pairing value for two integers x and y.\n",
    "    The Cantor pairing maps two integers to a unique integer.\n",
    "\n",
    "    Args:\n",
    "        x (int): The first integer.\n",
    "        y (int): The second integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The Cantor pairing value for the given integers x and y.\n",
    "    \"\"\"\n",
    "    return (x + y) * (x + y + 1) // 2 + y\n",
    "\n",
    "def cantor_encode_tensor(tnsr):\n",
    "    \"\"\"\n",
    "    Encodes a list of integers in tensor rows into single integers using Cantor pairing.\n",
    "    The function recursively applies the Cantor pairing function to pairs of elements\n",
    "    in the input tensor until it encodes the entire list into a single integer.\n",
    "\n",
    "    If the input tensor is empty, the function returns 0.\n",
    "\n",
    "    Args:\n",
    "        tnsr (torch.Tensor): A tensor containing a list of integers.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Cantor encoded integer representing the input list.\n",
    "    \"\"\"\n",
    "    if tnsr.size(1) == 0:\n",
    "        # Termination point. The added 0 has no effect on the returned integer\n",
    "        return torch.tensor(0)\n",
    "    else:\n",
    "        return cantor_pairing(tnsr[:, 0], cantor_encode_tensor(tnsr[:, 1:]))\n",
    "\n",
    "\n",
    "class WalksDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to handle sequences of node walks.\n",
    "\n",
    "    Args:\n",
    "        dag_data (object): The input DAG data.\n",
    "        dict_cantor_to_honode_ixs_mapping (dict): Dictionary mapping Cantor encoded indices to higher-order node indices.\n",
    "        max_order (int): Maximum order of nodes in a walk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dag_data, dict_cantor_to_honode_ixs_mapping, max_order):\n",
    "        self.max_order = max_order\n",
    "        self.dict_cantor_to_honode_ixs_mapping = dict_cantor_to_honode_ixs_mapping\n",
    "        self._preprocess_data(dag_data.dags)\n",
    "        self._create_tensors_and_encodings()\n",
    "\n",
    "    def _preprocess_data(self, dags):\n",
    "        \"\"\"\n",
    "        Preprocesses the DAG data to extract walks and their counts.\n",
    "\n",
    "        Args:\n",
    "            dags DAGData object\n",
    "        \"\"\"\n",
    "        self.walks_by_length = {}\n",
    "        self.walk_counts_by_length = {}\n",
    "        self.total_sequences = 0\n",
    "        \n",
    "        for dag in dags:\n",
    "            node_seq_path = dag.node_sequence.T[0]\n",
    "            seq_length = len(node_seq_path)\n",
    "            if seq_length not in self.walks_by_length:\n",
    "                self.walks_by_length[seq_length] = []\n",
    "                self.walk_counts_by_length[seq_length] = []\n",
    "            self.walks_by_length[seq_length].append(node_seq_path)\n",
    "            # Answer to Moritz question\n",
    "            # probably one weight per DAG would be enough\n",
    "            self.walk_counts_by_length[seq_length].append(int(dag.edge_weight.unique()))\n",
    "            self.total_sequences += 1\n",
    "\n",
    "    def _create_tensors_and_encodings(self):\n",
    "        \"\"\"\n",
    "        Creates tensors and encodings for the walks.\n",
    "        \"\"\"\n",
    "        self.walk_tensors_by_length = {\n",
    "            length: torch.stack(walks, dim=0)\n",
    "            for length, walks in self.walks_by_length.items()\n",
    "        }\n",
    "        self.bipartite_encoded_walks_by_length = {\n",
    "            length: self._bipartite_encode(length)\n",
    "            for length in self.walk_tensors_by_length\n",
    "        }\n",
    "\n",
    "    def _bipartite_encode(self, walk_length):\n",
    "        \"\"\"\n",
    "        Encodes the walks in bipartite form, i.e.,\n",
    "        representing transitions between indexes of higher-order nodes.\n",
    "\n",
    "        In bipartite encoding, each transition in the walk sequence corresponds to a pair of indexes,\n",
    "        where the first `self.max_order` transitions utilize indexes from the higher-order nodes of the i-th order tensors, and the subsequent transitions use indexes of the `self.max_order` most recent nodes.\n",
    "\n",
    "        Args:\n",
    "            walk_length (int): The length of the walk.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Bipartite-encoded walk sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        list_cantor_node_ixs_tensors = []\n",
    "        \n",
    "        for i in range(1, walk_length + 1):\n",
    "            hon_ixs_tensor = self.walk_tensors_by_length[walk_length][:, max(0, i - self.max_order):i]\n",
    "            cantor_encoded = cantor_encode_tensor(hon_ixs_tensor)\n",
    "            mapped_indices = cantor_encoded.apply_(self.dict_cantor_to_honode_ixs_mapping[min(i, self.max_order)].get)\n",
    "            list_cantor_node_ixs_tensors.append(mapped_indices)\n",
    "            \n",
    "        return torch.stack(list_cantor_node_ixs_tensors, dim=1)\n",
    "\n",
    "    def __getitem__(self, index_tuple):\n",
    "        \"\"\"\n",
    "        Retrieves a bipartite-encoded node sequence from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index_tuple (tuple): A tuple containing the length of the sequence (int) and the index of the sequence within that length (int).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The bipartite-encoded node sequence.\n",
    "        \"\"\"\n",
    "        # This cannot work with batching. Could work if each walk length had its own Dataset object\n",
    "        walk_length, index = index_tuple\n",
    "        return self.bipartite_encoded_walks_by_length[walk_length][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of sequences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of sequences.\n",
    "        \"\"\"\n",
    "        return self.total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_data = pp.DAGData(pp.IndexMap(list(\"01234\")))\n",
    "\n",
    "dag_data.append_walk(list(\"0230230\"), weight=30)\n",
    "dag_data.append_walk(list(\"1241241\"), weight=70)\n",
    "dag_data.append_walk(list(\"0230241\"), weight=1)\n",
    "#\n",
    "# dag_data.append_walk(list(\"0230230230230230230\"), weight=30)\n",
    "\n",
    "\n",
    "m = pp.MultiOrderModel.from_DAGs(dag_data, max_order=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cantor_to_honode_ixs_mapping = {}\n",
    "# This works cause the ho-node sequences are sorted by their indices\n",
    "for order, hon in m.layers.items():\n",
    "    cantor_ids = cantor_encode_tensor(hon.data.node_sequence)\n",
    "    cantor_to_node_ixs_mapping = {cantor_id.item(): i for i, cantor_id in enumerate(cantor_ids)}\n",
    "    dict_cantor_to_honode_ixs_mapping[order] = cantor_to_node_ixs_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_data = WalksDataset(dag_data, dict_cantor_to_honode_ixs_mapping, max_order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: following cells contraints all this to be done on a single value of path length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =7\n",
    "source_to_target_from_walks = walk_data.bipartite_encoded_walks_by_length[l]\n",
    "path_counts = walk_data.walk_counts_by_length[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THIS!\n",
    "# FIND ANOTHER WAY TO GET NODE COUNTS\n",
    "from torch_geometric.loader import DataLoader\n",
    "dag_graph = next(iter(DataLoader(dag_data.dags, batch_size=len(dag_data.dags)))).to(pp.config[\"torch\"][\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-739.3706)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_nodes, counts = torch.unique(dag_graph.node_sequence, return_counts=True)\n",
    "node_emission_probabilities = counts / counts.sum()\n",
    "\n",
    "source_to_target_edge_index_zeroth = torch.stack([\n",
    "    torch.zeros_like(source_to_target_from_walks[:, 0]),\n",
    "    source_to_target_from_walks[:, 0]\n",
    "])\n",
    "\n",
    "# log likelihood\n",
    "tot_log_lh = 0\n",
    "# log likelihood for the 0-th steps\n",
    "lh_l = torch.mul(torch.log(node_emission_probabilities[source_to_target_edge_index_zeroth[1]]), torch.tensor(path_counts))\n",
    "tot_log_lh += lh_l.sum()\n",
    "\n",
    "for i in range(0, l - 1):\n",
    "    T = compute_transition_probabilities(m.layers[min(i + 2, walk_data.max_order)])\n",
    "    # Prepare source_to_target_edge_index\n",
    "    source_to_target_edge_index = source_to_target_from_walks[:, i:i + 2].T.squeeze()\n",
    "    # log likelihood for i-th steps\n",
    "    lh_l = torch.mul(torch.log(T[source_to_target_edge_index[1]]), torch.tensor(path_counts))\n",
    "    tot_log_lh += lh_l.sum()\n",
    "\n",
    "tot_log_lh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adapt to multiple walk lengths\n",
    "- Better structure for package\n",
    "- Degrees of freedom etc (actually making the likilhood ration test test)\n",
    "    - This all thing above is getting lhj\n",
    "    - Need degrees of freedom\n",
    "    - Need to actually perform che likelihood ratio test (see line 407 of this: https://github.com/IngoScholtes/pathpy/blob/master/pathpy/MultiOrderModel.py#L378). \n",
    "- test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_mon_dof(m,max_order, assumption = \"paths\"):\n",
    "    \"\"\"\n",
    "    The degrees of freedom fo the kth layer of a multi-order model this depende on the number of different paths of exactly length k in the graph.\n",
    "    Therefore, we can obtain this values by summing the entries of the kth power of the binary adhacency matrix of the graph.\n",
    "    Finally, we must consider that, due the conservation of probablility, all non-zero rows of the transition matrix of the higher-order network must sum to one. \n",
    "    This poses on additional constraint per row that respects the condition, which should be removed from the total count of degrees of freedom.\n",
    "\n",
    "    @param: maxOrder: the maximum order up to which model layers shall be\n",
    "    taken into account\n",
    "\n",
    "    @param assumption: if set to 'paths', for the degree of freedom calculation\n",
    "        only paths in the first-order network topology will be considered. This is\n",
    "        needed whenever we model paths in a *given* network topology.\n",
    "        If set to 'ngrams' all possible n-grams will be considered, independent of whether they\n",
    "        are valid paths in the first-order network or not. The 'ngrams' and the 'paths' assumption\n",
    "        coincide if the first-order network is fully connected, i.e. if all possible paths actually occur.\n",
    "    \"\"\"\n",
    "    if max_order == None:\n",
    "        max_order = max(m.layers)\n",
    "    assert max_order <= max(m.layers), 'Error: maxOrder cannot be larger than maximum order of multi-order network'\n",
    "\n",
    "    # for both assumptions, the zeroth order simply gives the node probabilities. \n",
    "    dof = m.layers[1].data.num_nodes - 1\n",
    "\n",
    "    if assumption == \"paths\":\n",
    "        for order in range(1, max_order+1):\n",
    "            num_order_paths = m.layers[order].data.num_nodes\n",
    "            num_nonzero_outdegrees = len(m.layers[order].data.edge_index[0].unique())\n",
    "            dof += num_order_paths - num_nonzero_outdegrees\n",
    "    elif assumption == \"ngram\":\n",
    "        # the ngram assumption corresponds to the assumption normally made when using higher-order markov chains\n",
    "        # i.e., every node can follow every predecent sequence of nodes\n",
    "        for order in range(1, max_order + 1):\n",
    "            dof += (m.layers[1].data.num_nodes**order)*(m.layers[1].data.num_nodes - 1) \n",
    "    else:\n",
    "        assert False, f\"Unknown assumption {assumption} in input. The only accepted values are 'path' and 'ngram' \"\n",
    "    \n",
    "    return int(dof)\n",
    "\n",
    "get_mon_dof(m,max_order=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
