{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import scatter\n",
    "\n",
    "import pathpyG as pp\n",
    "pp.config['torch']['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_outdegrees(graph):\n",
    "    weighted_outdegree = scatter(graph.data.edge_weight, graph.data.edge_index[0], dim=0, dim_size=graph.data.num_nodes, reduce='sum')\n",
    "    return weighted_outdegree\n",
    "\n",
    "def compute_transition_probabilities(graph):\n",
    "    weighted_outdegree = compute_weighted_outdegrees(graph)\n",
    "    source_ids = graph.data.edge_index[0]\n",
    "    return graph.data.edge_weight/ weighted_outdegree[source_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Pairing_function\n",
    "# https://math.stackexchange.com/questions/1377929/generalization-of-cantor-pairing-function-to-triples-and-n-tuples\n",
    "def cantor_pairing(x, y):\n",
    "    \"\"\"\n",
    "    Computes the Cantor pairing value for two integers x and y.\n",
    "    The Cantor pairing maps two integers to a unique integer.\n",
    "\n",
    "    Args:\n",
    "        x (int): The first integer.\n",
    "        y (int): The second integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The Cantor pairing value for the given integers x and y.\n",
    "    \"\"\"\n",
    "    return (x + y) * (x + y + 1) // 2 + y\n",
    "\n",
    "def cantor_encode_tensor(tnsr):\n",
    "    \"\"\"\n",
    "    Encodes a list of integers in tensor rows into single integers using Cantor pairing.\n",
    "    The function recursively applies the Cantor pairing function to pairs of elements\n",
    "    in the input tensor until it encodes the entire list into a single integer.\n",
    "\n",
    "    If the input tensor is empty, the function returns 0.\n",
    "\n",
    "    Args:\n",
    "        tnsr (torch.Tensor): A tensor containing a list of integers.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Cantor encoded integer representing the input list.\n",
    "    \"\"\"\n",
    "    if tnsr.size(1) == 0:\n",
    "        # Termination point. The added 0 has no effect on the returned integer\n",
    "        return torch.tensor(0)\n",
    "    else:\n",
    "        return cantor_pairing(tnsr[:, 0], cantor_encode_tensor(tnsr[:, 1:]))\n",
    "\n",
    "\n",
    "class WalksDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to handle sequences of node walks.\n",
    "\n",
    "    Args:\n",
    "        dag_data (object): The input DAG data.\n",
    "        dict_cantor_to_honode_ixs_mapping (dict): Dictionary mapping Cantor encoded indices to higher-order node indices.\n",
    "        max_order (int): Maximum order of nodes in a walk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dag_data, dict_cantor_to_honode_ixs_mapping, max_order):\n",
    "        self.max_order = max_order\n",
    "        self.dict_cantor_to_honode_ixs_mapping = dict_cantor_to_honode_ixs_mapping\n",
    "        self._preprocess_data(dag_data.dags)\n",
    "        if max_order>0:\n",
    "            self._create_walk_tensors()\n",
    "            self._create_encoded_tensors()\n",
    "        else:\n",
    "            self._create_walk_tensors()\n",
    "            self.bipartite_encoded_walks_by_length = self.walk_tensors_by_length\n",
    "\n",
    "\n",
    "\n",
    "    def _preprocess_data(self, dags):\n",
    "        \"\"\"\n",
    "        Preprocesses the DAG data to extract walks and their counts.\n",
    "\n",
    "        Args:\n",
    "            dags DAGData object\n",
    "        \"\"\"\n",
    "        self.walks_by_length = {}\n",
    "        self.walk_counts_by_length = {}\n",
    "        self.total_sequences = 0\n",
    "        \n",
    "        for dag in dags:\n",
    "            node_seq_path = dag.node_sequence.T[0]\n",
    "            seq_length = len(node_seq_path)\n",
    "            if seq_length not in self.walks_by_length:\n",
    "                self.walks_by_length[seq_length] = []\n",
    "                self.walk_counts_by_length[seq_length] = []\n",
    "            self.walks_by_length[seq_length].append(node_seq_path)\n",
    "            # Answer to Moritz question?\n",
    "            # probably one weight per DAG would be enough\n",
    "            self.walk_counts_by_length[seq_length].append(int(dag.edge_weight.unique()))\n",
    "            self.total_sequences += 1\n",
    "\n",
    "    def _create_walk_tensors(self):\n",
    "        \"\"\"\n",
    "        Creates tensors for each walk lengths.\n",
    "        \"\"\"\n",
    "        self.walk_tensors_by_length = {\n",
    "            length: torch.stack(walks, dim=0)\n",
    "            for length, walks in self.walks_by_length.items()\n",
    "        }\n",
    "    def _create_encoded_tensors(self): # TODO: find better names for this method and the _bipartite_encode\n",
    "        \"\"\"\n",
    "        Encode the walk representations using cantor encoding\n",
    "        \"\"\"\n",
    "        self.bipartite_encoded_walks_by_length = {\n",
    "            length: self._bipartite_encode(length)\n",
    "            for length in self.walk_tensors_by_length\n",
    "        }\n",
    "\n",
    "    def _bipartite_encode(self, walk_length):\n",
    "        \"\"\"\n",
    "        Encodes the walks in bipartite form, i.e.,\n",
    "        representing transitions between indexes of higher-order nodes.\n",
    "\n",
    "        In bipartite encoding, each transition in the walk sequence corresponds to a pair of indexes (from nth to n+1th column),\n",
    "        where the first `self.max_order` transitions utilize indexes from the higher-order nodes of the i-th order tensors, \n",
    "        and the subsequent transitions use indexes of the `self.max_order` most recent nodes.\n",
    "\n",
    "        Args:\n",
    "            walk_length (int): The length of the walk.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Bipartite-encoded walk sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        list_cantor_node_ixs_tensors = []\n",
    "        \n",
    "        for i in range(0, walk_length): \n",
    "            hon_ixs_tensor = self.walk_tensors_by_length[walk_length][:, max(0, i - self.max_order):i+1] \n",
    "            cantor_encoded = cantor_encode_tensor(hon_ixs_tensor)\n",
    "            mapped_indices = cantor_encoded.apply_(self.dict_cantor_to_honode_ixs_mapping[min(i+1, self.max_order+1)].get) # +1 cause for first order we want edges etc.\n",
    "            list_cantor_node_ixs_tensors.append(mapped_indices)\n",
    "            \n",
    "        return torch.stack(list_cantor_node_ixs_tensors, dim=1)\n",
    "\n",
    "    def __getitem__(self, index_tuple):\n",
    "        \"\"\"\n",
    "        Retrieves a bipartite-encoded node sequence from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index_tuple (tuple): A tuple containing the length of the sequence (int) and the index of the sequence within that length (int).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The bipartite-encoded node sequence.\n",
    "        \"\"\"\n",
    "        # This cannot work with batching. Could work if each walk length had its own Dataset object\n",
    "        walk_length, index = index_tuple\n",
    "        return self.bipartite_encoded_walks_by_length[walk_length][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of sequences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of sequences.\n",
    "        \"\"\"\n",
    "        return self.total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_data = pp.DAGData(pp.IndexMap(list(\"01234\")))\n",
    "\n",
    "dag_data.append_walk(list(\"0230230230230\"), weight=300)\n",
    "dag_data.append_walk(list(\"1241241241241\"), weight=300)\n",
    "dag_data.append_walk(list(\"0430241\"), weight=1)\n",
    "\n",
    "m = pp.MultiOrderModel.from_DAGs(dag_data, max_order=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cantor_econding_mon_nodes(m):\n",
    "    \"\"\" \n",
    "    Applies cantor encoding to the node sequences of the higher-order nodes for all the layers of a MultiOrderModel\n",
    "    Returns a dictionary containing, for each layer, a mapping from the cantor index (encoding) of the sequence to the index of the ho-node in the Graph object\n",
    "    \"\"\"\n",
    "    # zeroth ixs are node ixs\n",
    "    dict_cantor_to_honode_ixs_mapping = {0:{i:i for i in range(m.layers[1].data.num_nodes)}}\n",
    "    # This works cause the ho-node sequences are sorted by their indices\n",
    "    for order, hon in m.layers.items():\n",
    "        cantor_ids = cantor_encode_tensor(hon.data.node_sequence)\n",
    "        cantor_to_node_ixs_mapping = {cantor_id.item(): i for i, cantor_id in enumerate(cantor_ids)}\n",
    "        dict_cantor_to_honode_ixs_mapping[order] = cantor_to_node_ixs_mapping # HEREEEE\n",
    "    return dict_cantor_to_honode_ixs_mapping\n",
    "dict_cantor_to_honode_ixs_mapping = get_cantor_econding_mon_nodes(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -12322.8798828125\n",
      "1 -2658.5830078125\n",
      "2 -993.5861206054688\n",
      "3 -993.5485229492188\n",
      "4 -985.7455444335938\n",
      "5 -985.7455444335938\n",
      "6 -985.7455444335938\n",
      "7 -985.7455444335938\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "def get_mon_likelihood(mon, walk_data, max_order=1):\n",
    "    \"\"\"\n",
    "    Compute the likelihood of the walks given a multi-order model.\n",
    "\n",
    "    Args:\n",
    "        mon (MultiOrderModel): The multi-order model.\n",
    "        walk_data (Dataset): Dataset containing the walks.\n",
    "        max_order (int, optional): The maximum order up to which model layers\n",
    "            shall be taken into account. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log likelihood of the walks given the multi-order model.\n",
    "    \"\"\"\n",
    "    assert max_order <= max(mon.layers), \"max_order for the walk_data cannot be larger than the max_order in the mon\"\n",
    "\n",
    "    # Get encoding for nodes in the multi-order model\n",
    "    dict_cantor_to_honode_ixs = get_cantor_econding_mon_nodes(mon)\n",
    "\n",
    "    # Prepare data for likelihood computation\n",
    "    inner_walk_data = WalksDataset(walk_data, dict_cantor_to_honode_ixs, max_order=max_order)\n",
    "\n",
    "    # Compute node emission probabilities for zeroth order\n",
    "    dag_graph = next(iter(DataLoader(walk_data.dags, batch_size=len(walk_data.dags)))).to(pp.config[\"torch\"][\"device\"])\n",
    "    _, counts = torch.unique(dag_graph.node_sequence, return_counts=True)\n",
    "    node_emission_probabilities = counts / counts.sum()\n",
    "\n",
    "    # Begin log likelihood computation\n",
    "    tot_log_lh = 0\n",
    "    for walk_length, bip_encoded_walks in inner_walk_data.bipartite_encoded_walks_by_length.items():\n",
    "        path_counts = inner_walk_data.walk_counts_by_length[walk_length]\n",
    "\n",
    "        # Compute log likelihood for higher orders\n",
    "        log_lh_path_length = torch.zeros(len(path_counts))\n",
    "        for i in range(0, walk_length):\n",
    "            if max_order == 0 or i == 0 :\n",
    "                T = node_emission_probabilities\n",
    "            else:\n",
    "                T = compute_transition_probabilities(m.layers[min(i,max_order)])\n",
    "                \n",
    "            target_indexes = bip_encoded_walks[:, i] \n",
    "            log_lh_path_length += torch.log(T[target_indexes])\n",
    "        tot_log_lh += torch.mul(log_lh_path_length, torch.tensor(path_counts)).sum()\n",
    "\n",
    "    return tot_log_lh.item()\n",
    "\n",
    "# # Compute likelihood for different maximum orders  \n",
    "for o in range(max(m.layers) ):\n",
    "    print(o, get_mon_likelihood(m, dag_data, max_order=o))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably there is somethign wrong in _bipartite_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dof 0 - 4\n",
      "dof 1 - 7\n",
      "dof 2 - 15\n",
      "dof 3 - 30\n",
      "dof 4 - 57\n",
      "dof 5 - 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/edge_index.py:784: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(\n"
     ]
    }
   ],
   "source": [
    "def get_mon_dof(m, max_order=None, assumption=\"paths\"):\n",
    "    \"\"\"\n",
    "    The degrees of freedom fo the kth layer of a multi-order model this depende on the number of different paths of exactly length k in the graph.\n",
    "    Therefore, we can obtain this values by summing the entries of the kth power of the binary adhacency matrix of the graph.\n",
    "    Finally, we must consider that, due the conservation of probablility, all non-zero rows of the transition matrix of the higher-order network must sum to one. \n",
    "    This poses on additional constraint per row that respects the condition, which should be removed from the total count of degrees of freedom.\n",
    "\n",
    "    Args:\n",
    "        m (MultiOrderModel): The multi-order model.\n",
    "        max_order (int, optional): The maximum order up to which model layers \n",
    "            shall be taken into account. Defaults to None, meaning it considers \n",
    "            all available layers.\n",
    "        assumption (str, optional): If set to 'paths', only paths in the \n",
    "            first-order network topology will be considered for the degree of \n",
    "            freedom calculation. If set to 'ngrams', all possible n-grams will \n",
    "            be considered, independent of whether they are valid paths in the \n",
    "            first-order network or not. Defaults to 'paths'.\n",
    "\n",
    "    Returns:\n",
    "        int: The degrees of freedom for the multi-order model.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If max_order is larger than the maximum order of \n",
    "            the multi-order network.\n",
    "        ValueError: If the assumption is not 'paths' or 'ngrams'.\n",
    "    \"\"\"\n",
    "    if max_order is None:\n",
    "        max_order = max(m.layers)\n",
    "    \n",
    "    assert max_order <= max(m.layers), \"Error: max_order cannot be larger than maximum order of multi-order network\"\n",
    "\n",
    "    dof = m.layers[1].data.num_nodes - 1  # Degrees of freedom for zeroth order\n",
    "\n",
    "    if assumption == \"paths\":\n",
    "        # COMPUTING CONTRIBUTION FROM NUM PATHS AND NONERO OUTDEGREES SEPARATELY\n",
    "        # TODO: CAN IT BE DONE TOGETHER?\n",
    "\n",
    "        # Adding dof from Number of paths of length k \n",
    "        for k in range(1, max_order + 1):\n",
    "            \n",
    "            if k == 1:\n",
    "                edge_index = m.layers[1].data.edge_index\n",
    "            else:\n",
    "                edge_index = m.lift_order_edge_index(edge_index, num_len_k_paths)\n",
    "            num_len_k_paths = edge_index.shape[1]  # Number of paths of length k\n",
    "            dof += num_len_k_paths \n",
    "        \n",
    "        # removing dof from total probability of nonzero degree nodes\n",
    "        for k in range(1, max_order+1):\n",
    "            \n",
    "            if k == 1:\n",
    "                edge_index_adj = m.layers[1].data.edge_index\n",
    "                edge_index = edge_index_adj\n",
    "            else:\n",
    "                edge_index, _ = edge_index @ edge_index_adj\n",
    "            num_nonzero_outdegrees = torch.unique(edge_index[0]).size(0)\n",
    "            dof -=  num_nonzero_outdegrees\n",
    "\n",
    "      \n",
    "    elif assumption == \"ngrams\":\n",
    "        for order in range(1, max_order + 1):\n",
    "            dof += (m.layers[1].data.num_nodes ** order) * (m.layers[1].data.num_nodes - 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown assumption {assumption} in input. The only accepted values are 'path' and 'ngram'\")\n",
    "\n",
    "    return int(dof)\n",
    "\n",
    "for o in range(0,6):\n",
    "    # get_mon_dof(m,max_order=o)\n",
    "    print(\"dof\",o,\"-\",get_mon_dof(m,max_order=o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "def lh_ratio_test(mon, walk_data, max_order_null = 0, max_order = 1, assumption='paths', significance_threshold=0.01):\n",
    "    assert max_order_null < max_order, 'Error: order of null hypothesis must be smaller than order of alternative hypothesis'\n",
    "    assert max_order < max(mon.layers), f'Error: order of hypotheses ({max_order_null} and {max_order}) must be smaller than the maximum order of the MultiOrderModel {max(mon.layers)}'\n",
    "    # let L0 be the likelihood for the null model and L1 be the likelihood for the alternative model\n",
    "\n",
    "    # we first compute a test statistic x = -2 * log (L0/L1) = -2 * (log L0 - log L1)\n",
    "    # get_mon_likelihood(mon, walk_data)\n",
    "    x = -2 * (get_mon_likelihood(mon, walk_data, max_order=max_order_null) - get_mon_likelihood(mon, walk_data, max_order=max_order))\n",
    "\n",
    "    # we calculate the additional degrees of freedom in the alternative model\n",
    "    dof_diff = get_mon_dof(m,max_order, assumption = assumption) - get_mon_dof(m,max_order_null, assumption = assumption)\n",
    "    print(x, dof_diff)\n",
    "\n",
    "    # if the p-value is *below* the significance threshold, we reject the null hypothesis\n",
    "    p = 1-chi2.cdf(x, dof_diff)\n",
    "    return (p<significance_threshold), p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mess originates from the fact that mon and walk dataset can have incompatbile values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3329.9937744140625 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_ratio_test(m, dag_data, max_order_null = 1, max_order = 2, assumption='paths', significance_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3329.9937744140625 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0751953125 15\n",
      "15.60595703125 27\n",
      "-0.0 47\n",
      "-0.0 76\n",
      "-0.0 124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_order(mon, walk_data, max_order=None, significance_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Selects the optimal maximum order of a multi-order network model for the\n",
    "    observed paths, based on a likelihood ratio test with p-value threshold of p\n",
    "    By default, all orders up to the maximum order of the multi-order model will be tested.\n",
    "\n",
    "    @param paths: The path statistics for which to perform the order selection\n",
    "\n",
    "    @param maxOrder: The maximum order up to which the multi-order model shall be tested.\n",
    "    \"\"\"\n",
    "    if max_order == None:\n",
    "        max_order = mon.max_order\n",
    "    assert max_order <= max(mon.layers), 'Error: maxOrder cannot be larger than maximum order of multi-order network'\n",
    "    assert max_order > 1, 'Error: maxOrder must be larger than one'\n",
    "\n",
    "    max_accepted_order = 1\n",
    "\n",
    "    # Test for highest order that passes\n",
    "    # likelihood ratio test against null model\n",
    "    for k in range(2, max_order+1):\n",
    "        if lh_ratio_test(m, walk_data, max_order_null = k-1, max_order = k, significance_threshold=significance_threshold)[0]:\n",
    "            max_accepted_order = k\n",
    "\n",
    "    return max_accepted_order\n",
    "\n",
    "estimate_order(m, dag_data, max_order=max(m.layers)-1) # this -1 indicates that something is still off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
