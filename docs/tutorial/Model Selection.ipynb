{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "import pathpyG as pp\n",
    "pp.config['torch']['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- turn into class functions: (the ones needed to estimate order)\n",
    "- add as utils (computing of weigthed outdegrees and transition probabobilities)\n",
    "- unit tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dag_data = pp.DAGData(pp.IndexMap(list(\"abcde\")))\n",
    "dag_data = pp.DAGData(pp.IndexMap(list(\"01234\")))\n",
    "\n",
    "# walk_1  =('a','b','c','d','e','c','b','a','c','d','e','c','e','d','c','a')\n",
    "# walk_2  =('a','b','c','d','e','c')\n",
    "# dag_data.append_walk(walk_1)\n",
    "# dag_data.append_walk(walk_2)\n",
    "\n",
    "\n",
    "dag_data.append_walk(list(\"0230230230230\"), weight=2)\n",
    "dag_data.append_walk(list(\"1241241241241\"), weight=2)\n",
    "# dag_data.append_walk(list(\"0430241\"), weight=1)\n",
    "\n",
    "m = pp.MultiOrderModel.from_DAGs(dag_data, max_order=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "dag_graph = next(iter(DataLoader(dag_data.dags, batch_size=len(dag_data.dags)))).to(pp.config[\"torch\"][\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_weighted_outdegrees(graph):\n",
    "    \"\"\"\n",
    "    Compute the weighted outdegrees of each node in the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (Graph): pathpy graph object.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Weighted outdegrees of nodes.\n",
    "    \"\"\"\n",
    "    weighted_outdegree = scatter(graph.data.edge_weight, graph.data.edge_index[0], dim=0, dim_size=graph.data.num_nodes, reduce='sum')\n",
    "    return weighted_outdegree\n",
    "\n",
    "def compute_transition_probabilities(graph):\n",
    "    \"\"\"\n",
    "    Compute transition probabilities based on weighted outdegrees.\n",
    "\n",
    "    Args:\n",
    "        graph (Graph): pathpy graph object.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Transition probabilities.\n",
    "    \"\"\"\n",
    "    weighted_outdegree = compute_weighted_outdegrees(graph)\n",
    "    source_ids = graph.data.edge_index[0]\n",
    "    return graph.data.edge_weight / weighted_outdegree[source_ids]\n",
    "\n",
    "def get_zeroth_order_log_likelihood(dag_graph): \n",
    "    \"\"\"\n",
    "    Compute the zeroth order log likelihood.\n",
    "\n",
    "    Args:\n",
    "        dag_graph (DataBatch): Input DAG graph data.\n",
    "\n",
    "    Returns:\n",
    "        float: Zeroth order log likelihood.\n",
    "    \"\"\"\n",
    "    # Get frequencies\n",
    "    # TODO: put this tensor directly in dag_graph (intead of edge_weight) and remove the following line\n",
    "    frequencies = dag_graph.edge_weight[dag_graph.ptr[:-1]]\n",
    "    \n",
    "    # Get ixs starting nodes\n",
    "    mask = torch.ones(dag_graph.num_nodes, dtype=bool) \n",
    "    mask[dag_graph.edge_index[1]] = False\n",
    "    start_ixs = dag_graph.node_sequence.squeeze()[mask]\n",
    "\n",
    "    # Compute node emission probabilities\n",
    "    # TODOL modify once we have zeroth order in mon\n",
    "    _, counts = torch.unique(dag_graph.node_sequence, return_counts=True)\n",
    "    node_emission_probabilities = counts / counts.sum()\n",
    "    return torch.mul(frequencies, torch.log(node_emission_probabilities[start_ixs])).sum().item()\n",
    "\n",
    "def get_intermediate_order_log_likelihood(m, dag_graph, order):\n",
    "    \"\"\"\n",
    "    Compute the intermediate order log likelihood.\n",
    "\n",
    "    Args:\n",
    "        m (MultiOrderModel): Multi-order model.\n",
    "        dag_graph (DataBatch): Input DAG graph data.\n",
    "        order (int): Order of the intermediate log likelihood.\n",
    "\n",
    "    Returns:\n",
    "        float: Intermediate order log likelihood.\n",
    "    \"\"\"\n",
    "    # Get frequencies\n",
    "    # TODO: put this tensor directly in dag_graph (intead of edge_weight) and remove the following line\n",
    "    frequencies = dag_graph.edge_weight[dag_graph.ptr[:-1]]\n",
    "    \n",
    "    # Get intermediate HO nodes ixs (indentify ixs of nodes without indegree)\n",
    "    mask = torch.ones(dag_graph.num_nodes, dtype=bool) \n",
    "    mask[dag_graph.edge_index[1]] = False\n",
    "    ixs = torch.where(mask)[0]\n",
    "    num_ixs = ixs.shape[0]\n",
    "    ho_intermediate_ixs = ixs + torch.arange(num_ixs) * order\n",
    "    print(dag_graph.edge_index.shape, m.layers[order+1].data.inverse_idx.shape)\n",
    "    m.layers[order+1].data.inverse_idx[ho_intermediate_ixs]\n",
    "    log_lh_unique_sub_walks =  torch.log(compute_transition_probabilities(m.layers[order])[m.layers[order+1].data.inverse_idx[ho_intermediate_ixs]])\n",
    "    return torch.mul(frequencies, log_lh_unique_sub_walks).sum().item()\n",
    "\n",
    "def get_mon_log_likelihood(m, dag_graph, max_order=1):\n",
    "    \"\"\"\n",
    "    Compute the likelihood of the walks given a multi-order model.\n",
    "\n",
    "    Args:\n",
    "        m (MultiOrderModel): The multi-order model.\n",
    "        dag_graph (DataBatch): Dataset containing the walks.\n",
    "        max_order (int, optional): The maximum order up to which model layers\n",
    "            shall be taken into account. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log likelihood of the walks given the multi-order model.\n",
    "    \"\"\"\n",
    "    llh = 0\n",
    "\n",
    "    # Adding likelihood of zeroth order\n",
    "    llh += get_zeroth_order_log_likelihood(dag_graph)\n",
    "\n",
    "    # Adding the likelihood for all the intermediate orders\n",
    "    for order in range(1, max_order):\n",
    "        llh += get_intermediate_order_log_likelihood(m, dag_graph, order)\n",
    "\n",
    "    # Adding the likelihood of highest/stationary order\n",
    "    if max_order > 0:\n",
    "        llh += (torch.log(compute_transition_probabilities(m.layers[max_order])) * m.layers[max_order].data.edge_weight).sum().item()\n",
    "    else:\n",
    "        # Compute likelihood for zeroth order (to be modified)\n",
    "        # TODO: modify once we have zeroth order in mon \n",
    "        # (then won t need to compute emission probs from dag_graph -- which also hinders us from computing the lh that a new set of path swas generated by the model)\n",
    "        frequencies = dag_graph.edge_weight[dag_graph.ptr[:-1]]\n",
    "        counts = torch.bincount(dag_graph.node_sequence.T[0], frequencies.repeat_interleave(dag_graph.ptr[1:] - dag_graph.ptr[:-1]))\n",
    "        node_emission_probabilities = counts / counts.sum()\n",
    "        llh = torch.mul(torch.log(node_emission_probabilities), counts).sum().item()\n",
    "\n",
    "    return llh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 12])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 0\n",
    "mask = torch.ones(dag_graph.num_nodes, dtype=bool) \n",
    "mask[dag_graph.edge_index[1]] = False\n",
    "ixs = torch.where(mask)[0]\n",
    "num_ixs = ixs.shape[0]\n",
    "ho_intermediate_ixs = ixs + torch.arange(num_ixs) * (order - 1) \n",
    "ho_intermediate_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -81.78048728363943\n",
      "1 -17.68498945236206\n",
      "torch.Size([2, 24]) torch.Size([24])\n",
      "2 -6.594634532928467\n",
      "torch.Size([2, 24]) torch.Size([24])\n",
      "torch.Size([2, 24]) torch.Size([22])\n",
      "3 -6.594634532928467\n",
      "torch.Size([2, 24]) torch.Size([24])\n",
      "torch.Size([2, 24]) torch.Size([22])\n",
      "torch.Size([2, 24]) torch.Size([20])\n",
      "4 -6.594634532928467\n",
      "torch.Size([2, 24]) torch.Size([24])\n",
      "torch.Size([2, 24]) torch.Size([22])\n",
      "torch.Size([2, 24]) torch.Size([20])\n",
      "torch.Size([2, 24]) torch.Size([18])\n",
      "5 -6.594634532928467\n",
      "torch.Size([2, 24]) torch.Size([24])\n",
      "torch.Size([2, 24]) torch.Size([22])\n",
      "torch.Size([2, 24]) torch.Size([20])\n",
      "torch.Size([2, 24]) torch.Size([18])\n",
      "torch.Size([2, 24]) torch.Size([16])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of bounds for dimension 0 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):  \n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(o,\u001b[43mget_mon_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdag_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[125], line 102\u001b[0m, in \u001b[0;36mget_mon_log_likelihood\u001b[0;34m(m, dag_graph, max_order)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Adding the likelihood for all the intermediate orders\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m order \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_order):\n\u001b[0;32m--> 102\u001b[0m     llh \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_intermediate_order_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdag_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Adding the likelihood of highest/stationary order\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_order \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[125], line 78\u001b[0m, in \u001b[0;36mget_intermediate_order_log_likelihood\u001b[0;34m(m, dag_graph, order)\u001b[0m\n\u001b[1;32m     76\u001b[0m ho_intermediate_ixs \u001b[38;5;241m=\u001b[39m ixs \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(num_ixs) \u001b[38;5;241m*\u001b[39m order\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(dag_graph\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mshape, m\u001b[38;5;241m.\u001b[39mlayers[order\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39minverse_idx\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 78\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mho_intermediate_ixs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     79\u001b[0m log_lh_unique_sub_walks \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mlog(compute_transition_probabilities(m\u001b[38;5;241m.\u001b[39mlayers[order])[m\u001b[38;5;241m.\u001b[39mlayers[order\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39minverse_idx[ho_intermediate_ixs]])\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmul(frequencies, log_lh_unique_sub_walks)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18 is out of bounds for dimension 0 with size 16"
     ]
    }
   ],
   "source": [
    "for o in range(max(m.layers)+1):  \n",
    "    print(o,get_mon_log_likelihood(m,dag_graph, max_order=o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably there is somethign wrong in _bipartite_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dof 0 - 4\n",
      "dof 1 - 5\n",
      "dof 2 - 8\n",
      "dof 3 - 13\n",
      "dof 4 - 20\n",
      "dof 5 - 31\n",
      "dof 6 - 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/edge_index.py:784: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(\n"
     ]
    }
   ],
   "source": [
    "def get_mon_dof(m, max_order=None, assumption=\"paths\"):\n",
    "    \"\"\"\n",
    "    The degrees of freedom fo the kth layer of a multi-order model this depende on the number of different paths of exactly length k in the graph.\n",
    "    Therefore, we can obtain this values by summing the entries of the kth power of the binary adhacency matrix of the graph.\n",
    "    Finally, we must consider that, due the conservation of probablility, all non-zero rows of the transition matrix of the higher-order network must sum to one. \n",
    "    This poses on additional constraint per row that respects the condition, which should be removed from the total count of degrees of freedom.\n",
    "\n",
    "    Args:\n",
    "        m (MultiOrderModel): The multi-order model.\n",
    "        max_order (int, optional): The maximum order up to which model layers \n",
    "            shall be taken into account. Defaults to None, meaning it considers \n",
    "            all available layers.\n",
    "        assumption (str, optional): If set to 'paths', only paths in the \n",
    "            first-order network topology will be considered for the degree of \n",
    "            freedom calculation. If set to 'ngrams', all possible n-grams will \n",
    "            be considered, independent of whether they are valid paths in the \n",
    "            first-order network or not. Defaults to 'paths'.\n",
    "\n",
    "    Returns:\n",
    "        int: The degrees of freedom for the multi-order model.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If max_order is larger than the maximum order of \n",
    "            the multi-order network.\n",
    "        ValueError: If the assumption is not 'paths' or 'ngrams'.\n",
    "    \"\"\"\n",
    "    if max_order is None:\n",
    "        max_order = max(m.layers)\n",
    "    \n",
    "    assert max_order <= max(m.layers), \"Error: max_order cannot be larger than maximum order of multi-order network\"\n",
    "\n",
    "    dof = m.layers[1].data.num_nodes - 1  # Degrees of freedom for zeroth order\n",
    "\n",
    "    if assumption == \"paths\":\n",
    "        # COMPUTING CONTRIBUTION FROM NUM PATHS AND NONERO OUTDEGREES SEPARATELY\n",
    "        # TODO: CAN IT BE DONE TOGETHER?\n",
    "\n",
    "        # Adding dof from Number of paths of length k \n",
    "        for k in range(1, max_order + 1):\n",
    "            \n",
    "            if k == 1:\n",
    "                edge_index = m.layers[1].data.edge_index\n",
    "            else:\n",
    "                edge_index = m.lift_order_edge_index(edge_index, num_len_k_paths)\n",
    "            num_len_k_paths = edge_index.shape[1]  # Number of paths of length k\n",
    "            dof += num_len_k_paths \n",
    "        \n",
    "        # removing dof from total probability of nonzero degree nodes\n",
    "        for k in range(1, max_order+1):\n",
    "            \n",
    "            if k == 1:\n",
    "                edge_index_adj = m.layers[1].data.edge_index\n",
    "                edge_index = edge_index_adj\n",
    "            else:\n",
    "                edge_index, _ = edge_index @ edge_index_adj\n",
    "            num_nonzero_outdegrees = torch.unique(edge_index[0]).size(0)\n",
    "            dof -=  num_nonzero_outdegrees\n",
    "\n",
    "      \n",
    "    elif assumption == \"ngrams\":\n",
    "        for order in range(1, max_order + 1):\n",
    "            dof += (m.layers[1].data.num_nodes ** order) * (m.layers[1].data.num_nodes - 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown assumption {assumption} in input. The only accepted values are 'path' and 'ngram'\")\n",
    "\n",
    "    return int(dof)\n",
    "\n",
    "for o in range(0,max(m.layers)+1):\n",
    "    # get_mon_dof(m,max_order=o)\n",
    "    print(\"dof\",o,\"-\",get_mon_dof(m,max_order=o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "def lh_ratio_test(mon, dag_graph, max_order_null = 0, max_order = 1, assumption='paths', significance_threshold=0.01):\n",
    "    assert max_order_null < max_order, 'Error: order of null hypothesis must be smaller than order of alternative hypothesis'\n",
    "    assert max_order < max(mon.layers), f'Error: order of hypotheses ({max_order_null} and {max_order}) must be smaller than the maximum order of the MultiOrderModel {max(mon.layers)}'\n",
    "    # let L0 be the likelihood for the null model and L1 be the likelihood for the alternative model\n",
    "\n",
    "    # we first compute a test statistic x = -2 * log (L0/L1) = -2 * (log L0 - log L1)\n",
    "    x = -2 * (get_mon_log_likelihood(mon, dag_graph, max_order=max_order_null) - get_mon_log_likelihood(mon, dag_graph, max_order=max_order))\n",
    "\n",
    "    # we calculate the additional degrees of freedom in the alternative model\n",
    "    dof_diff = get_mon_dof(m,max_order, assumption = assumption) - get_mon_dof(m,max_order_null, assumption = assumption)\n",
    "    print(x, dof_diff)\n",
    "\n",
    "    # if the p-value is *below* the significance threshold, we reject the null hypothesis\n",
    "    p = 1-chi2.cdf(x, dof_diff)\n",
    "    return (p<significance_threshold), p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mess originates from the fact that mon and walk dataset can have incompatbile values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.19099566255474 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_ratio_test(m, dag_graph, max_order_null = 0, max_order = 1, assumption='paths', significance_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.180709838867188 3\n",
      "-0.0 5\n",
      "-0.0 7\n",
      "-0.0 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_order(mon, walk_data, max_order=None, significance_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Selects the optimal maximum order of a multi-order network model for the\n",
    "    observed paths, based on a likelihood ratio test with p-value threshold of p\n",
    "    By default, all orders up to the maximum order of the multi-order model will be tested.\n",
    "\n",
    "    @param paths: The path statistics for which to perform the order selection\n",
    "\n",
    "    @param maxOrder: The maximum order up to which the multi-order model shall be tested.\n",
    "    \"\"\"\n",
    "    if max_order == None:\n",
    "        max_order = mon.max_order\n",
    "    assert max_order <= max(mon.layers), 'Error: maxOrder cannot be larger than maximum order of multi-order network'\n",
    "    assert max_order > 1, 'Error: maxOrder must be larger than one'\n",
    "\n",
    "    max_accepted_order = 1\n",
    "\n",
    "    # Test for highest order that passes\n",
    "    # likelihood ratio test against null model\n",
    "    for k in range(2, max_order+1):\n",
    "        if lh_ratio_test(m, walk_data, max_order_null = k-1, max_order = k, significance_threshold=significance_threshold)[0]:\n",
    "            max_accepted_order = k\n",
    "\n",
    "    return max_accepted_order\n",
    "\n",
    "estimate_order(m, dag_graph, max_order=max(m.layers)-1) # this -1 indicates that something is still off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
