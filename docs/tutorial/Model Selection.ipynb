{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "import pathpyG as pp\n",
    "pp.config['torch']['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_outdegrees(graph):\n",
    "    weighted_outdegree = scatter(graph.data.edge_weight, graph.data.edge_index[0], dim=0, dim_size=graph.data.num_nodes, reduce='sum')\n",
    "    return weighted_outdegree\n",
    "\n",
    "def compute_transition_probabilities(graph):\n",
    "    weighted_outdegree = compute_weighted_outdegrees(graph)\n",
    "    source_ids = graph.data.edge_index[0]\n",
    "    return graph.data.edge_weight/ weighted_outdegree[source_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Pairing_function\n",
    "# https://math.stackexchange.com/questions/1377929/generalization-of-cantor-pairing-function-to-triples-and-n-tuples\n",
    "def cantor_pairing(x, y):\n",
    "    \"\"\"\n",
    "    Computes the Cantor pairing value for two integers x and y.\n",
    "    The Cantor pairing maps two integers to a unique integer.\n",
    "\n",
    "    Args:\n",
    "        x (int): The first integer.\n",
    "        y (int): The second integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The Cantor pairing value for the given integers x and y.\n",
    "    \"\"\"\n",
    "    return (x + y) * (x + y + 1) // 2 + y\n",
    "\n",
    "def cantor_encode_tensor(tnsr):\n",
    "    \"\"\"\n",
    "    Encodes a list of integers in tensor rows into single integers using Cantor pairing.\n",
    "    The function recursively applies the Cantor pairing function to pairs of elements\n",
    "    in the input tensor until it encodes the entire list into a single integer.\n",
    "\n",
    "    If the input tensor is empty, the function returns 0.\n",
    "\n",
    "    Args:\n",
    "        tnsr (torch.Tensor): A tensor containing a list of integers.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Cantor encoded integer representing the input list.\n",
    "    \"\"\"\n",
    "    if tnsr.size(1) == 0:\n",
    "        # Termination point. The added 0 has no effect on the returned integer\n",
    "        return torch.tensor(0)\n",
    "    else:\n",
    "        return cantor_pairing(tnsr[:, 0], cantor_encode_tensor(tnsr[:, 1:]))\n",
    "\n",
    "\n",
    "class WalksDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to handle sequences of node walks.\n",
    "\n",
    "    Args:\n",
    "        dag_data (object): The input DAG data.\n",
    "        dict_cantor_to_honode_ixs_mapping (dict): Dictionary mapping Cantor encoded indices to higher-order node indices.\n",
    "        max_order (int): Maximum order of nodes in a walk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dag_data, dict_cantor_to_honode_ixs_mapping, max_order):\n",
    "        self.max_order = max_order\n",
    "        self.dict_cantor_to_honode_ixs_mapping = dict_cantor_to_honode_ixs_mapping\n",
    "        self._preprocess_data(dag_data.dags)\n",
    "        if max_order>1:\n",
    "            self._create_walk_tensors()\n",
    "            self._create_encoded_tensors()\n",
    "        else:\n",
    "            self._create_walk_tensors()\n",
    "            self.bipartite_encoded_walks_by_length = self.walk_tensors_by_length\n",
    "\n",
    "\n",
    "\n",
    "    def _preprocess_data(self, dags):\n",
    "        \"\"\"\n",
    "        Preprocesses the DAG data to extract walks and their counts.\n",
    "\n",
    "        Args:\n",
    "            dags DAGData object\n",
    "        \"\"\"\n",
    "        self.walks_by_length = {}\n",
    "        self.walk_counts_by_length = {}\n",
    "        self.total_sequences = 0\n",
    "        \n",
    "        for dag in dags:\n",
    "            node_seq_path = dag.node_sequence.T[0]\n",
    "            seq_length = len(node_seq_path)\n",
    "            if seq_length not in self.walks_by_length:\n",
    "                self.walks_by_length[seq_length] = []\n",
    "                self.walk_counts_by_length[seq_length] = []\n",
    "            self.walks_by_length[seq_length].append(node_seq_path)\n",
    "            # Answer to Moritz question\n",
    "            # probably one weight per DAG would be enough\n",
    "            self.walk_counts_by_length[seq_length].append(int(dag.edge_weight.unique()))\n",
    "            self.total_sequences += 1\n",
    "\n",
    "    def _create_walk_tensors(self):\n",
    "        \"\"\"\n",
    "        Creates tensors for each walk lengths.\n",
    "        \"\"\"\n",
    "        self.walk_tensors_by_length = {\n",
    "            length: torch.stack(walks, dim=0)\n",
    "            for length, walks in self.walks_by_length.items()\n",
    "        }\n",
    "    def _create_encoded_tensors(self): # TODO: find better names for this method and the _bipartite_encode\n",
    "        \"\"\"\n",
    "        Encode the walk representations using cantor encoding\n",
    "        \"\"\"\n",
    "        self.bipartite_encoded_walks_by_length = {\n",
    "            length: self._bipartite_encode(length)\n",
    "            for length in self.walk_tensors_by_length\n",
    "        }\n",
    "\n",
    "    def _bipartite_encode(self, walk_length):\n",
    "        \"\"\"\n",
    "        Encodes the walks in bipartite form, i.e.,\n",
    "        representing transitions between indexes of higher-order nodes.\n",
    "\n",
    "        In bipartite encoding, each transition in the walk sequence corresponds to a pair of indexes (from nth to n+1th column),\n",
    "        where the first `self.max_order` transitions utilize indexes from the higher-order nodes of the i-th order tensors, \n",
    "        and the subsequent transitions use indexes of the `self.max_order` most recent nodes.\n",
    "\n",
    "        Args:\n",
    "            walk_length (int): The length of the walk.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Bipartite-encoded walk sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        list_cantor_node_ixs_tensors = []\n",
    "        \n",
    "        for i in range(1, walk_length + 1):\n",
    "            hon_ixs_tensor = self.walk_tensors_by_length[walk_length][:, max(0, i - self.max_order):i]\n",
    "            cantor_encoded = cantor_encode_tensor(hon_ixs_tensor)\n",
    "            mapped_indices = cantor_encoded.apply_(self.dict_cantor_to_honode_ixs_mapping[min(i, self.max_order)].get)\n",
    "            list_cantor_node_ixs_tensors.append(mapped_indices)\n",
    "            \n",
    "        return torch.stack(list_cantor_node_ixs_tensors, dim=1)\n",
    "\n",
    "    def __getitem__(self, index_tuple):\n",
    "        \"\"\"\n",
    "        Retrieves a bipartite-encoded node sequence from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index_tuple (tuple): A tuple containing the length of the sequence (int) and the index of the sequence within that length (int).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The bipartite-encoded node sequence.\n",
    "        \"\"\"\n",
    "        # This cannot work with batching. Could work if each walk length had its own Dataset object\n",
    "        walk_length, index = index_tuple\n",
    "        return self.bipartite_encoded_walks_by_length[walk_length][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of sequences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of sequences.\n",
    "        \"\"\"\n",
    "        return self.total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_data = pp.DAGData(pp.IndexMap(list(\"01234\")))\n",
    "\n",
    "dag_data.append_walk(list(\"0230230230230\"), weight=300)\n",
    "dag_data.append_walk(list(\"1241241241241\"), weight=300)\n",
    "dag_data.append_walk(list(\"0430241\"), weight=1)\n",
    "#\n",
    "# dag_data.append_walk(list(\"0230230230230230230\"), weight=30)\n",
    "\n",
    "\n",
    "m = pp.MultiOrderModel.from_DAGs(dag_data, max_order=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cantor_econding_mon_nodes(m):\n",
    "    \"\"\" \n",
    "    Applies cantor encoding to the node sequences of the higher-order nodes for all the layers of a MultiOrderModel\n",
    "    Returns a dictionary containing, for each layer, a mapping from the cantor index (encoding) of the sequence to the index of the ho-node in the Graph object\n",
    "    \"\"\"\n",
    "    dict_cantor_to_honode_ixs_mapping = {}\n",
    "    # This works cause the ho-node sequences are sorted by their indices\n",
    "    for order, hon in m.layers.items():\n",
    "        cantor_ids = cantor_encode_tensor(hon.data.node_sequence)\n",
    "        cantor_to_node_ixs_mapping = {cantor_id.item(): i for i, cantor_id in enumerate(cantor_ids)}\n",
    "        dict_cantor_to_honode_ixs_mapping[order] = cantor_to_node_ixs_mapping\n",
    "        # print(order,hon.data.node_sequence.shape)\n",
    "        # print(cantor_to_node_ixs_mapping)\n",
    "    return dict_cantor_to_honode_ixs_mapping\n",
    "dict_cantor_to_honode_ixs_mapping = get_cantor_econding_mon_nodes(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{13: tensor([[0, 0, 0, 0, 0, 3, 5, 0, 3, 5, 0, 3, 5],\n",
       "         [1, 2, 3, 3, 2, 4, 7, 2, 4, 7, 2, 4, 7]]),\n",
       " 7: tensor([[0, 1, 2, 2, 1, 8, 6]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_data = WalksDataset(dag_data, dict_cantor_to_honode_ixs_mapping, max_order=5)\n",
    "walk_data.bipartite_encoded_walks_by_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: following cells contraints all this to be done on a single value of path length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = 7\n",
    "# source_to_target_from_walks = walk_data.bipartite_encoded_walks_by_length[l]\n",
    "# path_counts = walk_data.walk_counts_by_length[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND ANOTHER WAY TO GET NODE COUNTS\n",
    "from torch_geometric.loader import DataLoader\n",
    "dag_graph = next(iter(DataLoader(dag_data.dags, batch_size=len(dag_data.dags)))).to(pp.config[\"torch\"][\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_nodes, counts = torch.unique(dag_graph.node_sequence, return_counts=True)\n",
    "# node_emission_probabilities = counts / counts.sum()\n",
    "\n",
    "# source_to_target_edge_index_zeroth = torch.stack([\n",
    "#     torch.zeros_like(source_to_target_from_walks[:, 0]),\n",
    "#     source_to_target_from_walks[:, 0]\n",
    "# ])\n",
    "\n",
    "# # log likelihood\n",
    "# tot_log_lh = 0\n",
    "# # log likelihood for the 0-th steps\n",
    "# lh_l = torch.mul(torch.log(node_emission_probabilities[source_to_target_edge_index_zeroth[1]]), torch.tensor(path_counts))\n",
    "# tot_log_lh += lh_l.sum()\n",
    "\n",
    "# for i in range(1, l - 1):\n",
    "#     T = compute_transition_probabilities(m.layers[min(i + 1, walk_data.max_order)])\n",
    "#     # Prepare source_to_target_edge_index\n",
    "#     source_to_target_edge_index = source_to_target_from_walks[:, i:i + 2].T.squeeze()\n",
    "#     # log likelihood for i-th steps\n",
    "#     lh_l = torch.mul(torch.log(T[source_to_target_edge_index[1]]), torch.tensor(path_counts))\n",
    "#     tot_log_lh += lh_l.sum()\n",
    "\n",
    "# tot_log_lh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adapt to multiple walk lengths\n",
    "- Better structure for package\n",
    "- Degrees of freedom etc (actually making the likilhood ration test test)\n",
    "    - This all thing above is getting lh (organize)\n",
    "    - Need degrees of freedom\n",
    "    - Need to actually perform che likelihood ratio test (see line 407 of this: https://github.com/IngoScholtes/pathpy/blob/master/pathpy/MultiOrderModel.py#L378). \n",
    "- test functions \n",
    "    - Dof computation\n",
    "    - lh computation\n",
    "    - encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -11541.60546875\n",
      "1 -11541.60546875\n",
      "2 -2651.2412109375\n",
      "3 -1194.3135986328125\n",
      "4 -1194.5257568359375\n",
      "5 -1186.722900390625\n",
      "6 -1186.722900390625\n",
      "7 -1186.722900390625\n",
      "8 -1186.722900390625\n"
     ]
    }
   ],
   "source": [
    "def get_mon_likelihood(mon, walk_data, max_order = 1):\n",
    "    assert max_order <= max(mon.layers), \"max_order for the walk_data cannot be larger than the max_order in the mon\"\n",
    "    dict_cantor_to_honode_ixs = get_cantor_econding_mon_nodes(mon)\n",
    "    inner_walk_data = WalksDataset(walk_data, dict_cantor_to_honode_ixs, max_order=max_order) #max_order=max(mon.layers))\n",
    "\n",
    "    # zeroth order part:\n",
    "    # TODO: FIND ANOTHER WAY TO GET NODE COUNTS for node emission probabilities ###########\n",
    "    # notice that this is building the zeroth order of a MultiOrderModel\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    dag_graph = next(iter(DataLoader(dag_data.dags, batch_size=len(walk_data.dags)))).to(pp.config[\"torch\"][\"device\"])\n",
    "    __unique_nodes, counts = torch.unique(dag_graph.node_sequence, return_counts=True) # check if the nodes' order is always ok\n",
    "    node_emission_probabilities = counts / counts.sum()\n",
    "\n",
    "    # Beginnign of log likelihood computation\n",
    "    tot_log_lh = 0\n",
    "    for walk_length in inner_walk_data.walks_by_length:\n",
    "        # getting paths of length 'walk_length'\n",
    "        bip_encoded_walks = inner_walk_data.bipartite_encoded_walks_by_length[walk_length]       \n",
    "        path_counts = inner_walk_data.walk_counts_by_length[walk_length]\n",
    "        # log likelihood for the 0-th order\n",
    "        # necessary cause rigth now the 0th order is not in the mon\n",
    "        target_indexes = bip_encoded_walks[:,0]\n",
    "        lh_l = torch.mul(torch.log(node_emission_probabilities[target_indexes]), torch.tensor(path_counts))\n",
    "        tot_log_lh += lh_l.sum()\n",
    "        \n",
    "        # Above we got the node probabilities; therefore, here we start from edge probability\n",
    "        # Notice that this should also imply how the code above can be simplified\n",
    "        for i in range(2, walk_length):\n",
    "            if max_order>1:\n",
    "                T = compute_transition_probabilities(m.layers[min(i-1,max_order-1)]) # NB: max_order -1 shouldn t be ok though\n",
    "            else:\n",
    "                T = node_emission_probabilities\n",
    "            # Prepare source_to_target_edge_index\n",
    "            target_indexes = bip_encoded_walks[:, i] \n",
    "            # log likelihood for i-th steps\n",
    "            lh_l = torch.mul(torch.log(T[target_indexes]), torch.tensor(path_counts))\n",
    "            tot_log_lh += lh_l.sum()\n",
    "    return tot_log_lh.item()\n",
    "\n",
    "# print(max(m.layers))\n",
    "# cannot have that the max_order here is larger than the max_order in the mon\n",
    "for o in range(max(m.layers)+1):\n",
    "    print(o,get_mon_likelihood(m, dag_data, max_order=o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably there is somethign wrong in _bipartite_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "7\n",
      "15\n",
      "30\n",
      "57\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "def get_mon_dof(m, max_order=None, assumption=\"paths\"):\n",
    "    \"\"\"\n",
    "    The degrees of freedom fo the kth layer of a multi-order model this depende on the number of different paths of exactly length k in the graph.\n",
    "    Therefore, we can obtain this values by summing the entries of the kth power of the binary adhacency matrix of the graph.\n",
    "    Finally, we must consider that, due the conservation of probablility, all non-zero rows of the transition matrix of the higher-order network must sum to one. \n",
    "    This poses on additional constraint per row that respects the condition, which should be removed from the total count of degrees of freedom.\n",
    "\n",
    "    Args:\n",
    "        m (MultiOrderModel): The multi-order model.\n",
    "        max_order (int, optional): The maximum order up to which model layers \n",
    "            shall be taken into account. Defaults to None, meaning it considers \n",
    "            all available layers.\n",
    "        assumption (str, optional): If set to 'paths', only paths in the \n",
    "            first-order network topology will be considered for the degree of \n",
    "            freedom calculation. If set to 'ngrams', all possible n-grams will \n",
    "            be considered, independent of whether they are valid paths in the \n",
    "            first-order network or not. Defaults to 'paths'.\n",
    "\n",
    "    Returns:\n",
    "        int: The degrees of freedom for the multi-order model.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If max_order is larger than the maximum order of \n",
    "            the multi-order network.\n",
    "        ValueError: If the assumption is not 'paths' or 'ngrams'.\n",
    "    \"\"\"\n",
    "    if max_order is None:\n",
    "        max_order = max(m.layers)\n",
    "    \n",
    "    assert max_order <= max(m.layers), \"Error: max_order cannot be larger than maximum order of multi-order network\"\n",
    "\n",
    "    dof = m.layers[1].data.num_nodes - 1  # Degrees of freedom for zeroth order\n",
    "\n",
    "    if assumption == \"paths\":\n",
    "        # COMPUTING CONTRIBUTION FROM NUM PATHS AND NONERO OUTDEGREES SEPARATELY\n",
    "        # TODO: CAN IT BE DONE TOGETHER?\n",
    "\n",
    "        # Adding dof from Number of paths of length k \n",
    "        for k in range(1, max_order + 1):\n",
    "            \n",
    "            if k == 1:\n",
    "                edge_index = m.layers[1].data.edge_index\n",
    "            else:\n",
    "                edge_index = m.lift_order_edge_index(edge_index, num_len_k_paths)\n",
    "            num_len_k_paths = edge_index.shape[1]  # Number of paths of length k\n",
    "            dof += num_len_k_paths \n",
    "        \n",
    "        # removing dof from total probability of nonzero degree nodes\n",
    "        for k in range(1, max_order+1):\n",
    "            \n",
    "            if k == 1:\n",
    "                edge_index_adj = m.layers[1].data.edge_index\n",
    "                edge_index = edge_index_adj\n",
    "            else:\n",
    "                edge_index, _ = edge_index @ edge_index_adj\n",
    "            num_nonzero_outdegrees = torch.unique(edge_index[0]).size(0)\n",
    "            dof -=  num_nonzero_outdegrees\n",
    "\n",
    "      \n",
    "    elif assumption == \"ngrams\":\n",
    "        for order in range(1, max_order + 1):\n",
    "            dof += (m.layers[1].data.num_nodes ** order) * (m.layers[1].data.num_nodes - 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown assumption {assumption} in input. The only accepted values are 'path' and 'ngram'\")\n",
    "\n",
    "    return int(dof)\n",
    "\n",
    "for o in range(0,6):\n",
    "    # get_mon_dof(m,max_order=o)\n",
    "    print(get_mon_dof(m,max_order=o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "def lh_ratio_test(mon, walk_data, max_order_null = 0, max_order = 1, assumption='paths', significanceThreshold=0.01):\n",
    "    assert max_order_null < max_order, 'Error: order of null hypothesis must be smaller than order of alternative hypothesis'\n",
    "    assert max_order < max(mon.layers), f'Error: order of hypotheses ({max_order_null} and {max_order}) must be smaller than the maximum order of the MultiOrderModel {max(mon.layers)}'\n",
    "    # let L0 be the likelihood for the null model and L1 be the likelihood for the alternative model\n",
    "\n",
    "    # we first compute a test statistic x = -2 * log (L0/L1) = -2 * (log L0 - log L1)\n",
    "    # get_mon_likelihood(mon, walk_data)\n",
    "    x = -2 * (get_mon_likelihood(mon, walk_data, max_order=max_order_null) - get_mon_likelihood(mon, walk_data, max_order=max_order))\n",
    "\n",
    "    # we calculate the additional degrees of freedom in the alternative model\n",
    "    dof_diff = get_mon_dof(m,max_order, assumption = assumption) - get_mon_dof(m,max_order_null, assumption = assumption)\n",
    "    print(x, dof_diff)\n",
    "\n",
    "    # if the p-value is *below* the significance threshold, we reject the null hypothesis\n",
    "    p = 1-chi2.cdf(x, dof_diff)\n",
    "    return (p<significanceThreshold), p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mess originates from the fact that mon and walk dataset can have incompatbile values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 1, 2, 2, 3, 4, 4, 5, 5, 6, 7],\n",
      "        [3, 4, 6, 7, 3, 4, 5, 6, 7, 0, 1, 2, 5]])\n",
      "tensor([[ 0,  1,  1,  2,  3,  4,  5,  5,  6,  6,  7,  8,  9,  9, 10, 10, 11, 11,\n",
      "         12, 12],\n",
      "        [ 6,  7,  8, 11, 12,  6,  7,  8,  9, 10, 11, 12,  0,  1,  2,  3,  4,  5,\n",
      "          9, 10]])\n",
      "tensor([[ 0,  0,  1,  2,  3,  3,  4,  4,  5,  5,  6,  7,  8,  8,  9,  9, 10, 10,\n",
      "         11, 11, 12, 13, 13, 14, 15, 16, 17, 17, 18, 18, 19, 19],\n",
      "        [ 8,  9, 10, 11, 16, 17, 18, 19,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19,  0,  1,  2,  3,  4,  5,  6,  7, 12, 13, 14, 15]])\n",
      "tensor([[0, 0, 1, 1, 2, 2, 3, 4, 4, 5, 5, 6, 7],\n",
      "        [3, 4, 6, 7, 3, 4, 5, 6, 7, 0, 1, 2, 5]])\n",
      "tensor([[ 0,  1,  1,  2,  3,  4,  5,  5,  6,  6,  7,  8,  9,  9, 10, 10, 11, 11,\n",
      "         12, 12],\n",
      "        [ 6,  7,  8, 11, 12,  6,  7,  8,  9, 10, 11, 12,  0,  1,  2,  3,  4,  5,\n",
      "          9, 10]])\n",
      "-0.42431640625 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 1.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODOS\n",
    "# Deal with computation from 0th to 1st\n",
    "# Do the zeroth and first order really have the same degrees of freedom in this example? Notice how it leads to a nan. \n",
    "lh_ratio_test(m, dag_data, max_order_null = 3, max_order = 4, assumption='paths', significanceThreshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17780.728515625 8\n",
      "2913.855224609375 15\n",
      "-0.42431640625 27\n",
      "15.605712890625 47\n",
      "-0.0 76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_order(mon, walk_data, max_order=None, significanceThreshold=0.01):\n",
    "    \"\"\"\n",
    "    Selects the optimal maximum order of a multi-order network model for the\n",
    "    observed paths, based on a likelihood ratio test with p-value threshold of p\n",
    "    By default, all orders up to the maximum order of the multi-order model will be tested.\n",
    "\n",
    "    @param paths: The path statistics for which to perform the order selection\n",
    "\n",
    "    @param maxOrder: The maximum order up to which the multi-order model shall be tested.\n",
    "    \"\"\"\n",
    "    if max_order == None:\n",
    "        max_order = mon.max_order\n",
    "    assert max_order <= max(mon.layers), 'Error: maxOrder cannot be larger than maximum order of multi-order network'\n",
    "    assert max_order > 1, 'Error: maxOrder must be larger than one'\n",
    "\n",
    "    max_accepted_order = 1\n",
    "\n",
    "    # Test for highest order that passes\n",
    "    # likelihood ratio test against null model\n",
    "    for k in range(2, max_order+1):\n",
    "        if lh_ratio_test(m, walk_data, max_order_null = k-1, max_order = k, significanceThreshold=significanceThreshold)[0]:\n",
    "            max_accepted_order = k\n",
    "\n",
    "    return max_accepted_order\n",
    "\n",
    "estimate_order(m, dag_data, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
