{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "import pathpyG as pp\n",
    "pp.config['torch']['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_outdegrees(graph):\n",
    "    weighted_outdegree = scatter(graph.data.edge_weight, graph.data.edge_index[0], dim=0, dim_size=graph.data.num_nodes, reduce='sum')\n",
    "    return weighted_outdegree\n",
    "\n",
    "def compute_transition_probabilities(graph):\n",
    "    weighted_outdegree = compute_weighted_outdegrees(graph)\n",
    "    source_ids = graph.data.edge_index[0]\n",
    "    return graph.data.edge_weight/ weighted_outdegree[source_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Pairing_function\n",
    "# https://math.stackexchange.com/questions/1377929/generalization-of-cantor-pairing-function-to-triples-and-n-tuples\n",
    "def cantor_pairing(x, y):\n",
    "    \"\"\"\n",
    "    Computes the Cantor pairing value for two integers x and y.\n",
    "    The Cantor pairing maps two integers to a unique integer.\n",
    "\n",
    "    Args:\n",
    "        x (int): The first integer.\n",
    "        y (int): The second integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The Cantor pairing value for the given integers x and y.\n",
    "    \"\"\"\n",
    "    return (x + y) * (x + y + 1) // 2 + y\n",
    "\n",
    "def cantor_encode_tensor(tnsr):\n",
    "    \"\"\"\n",
    "    Encodes a list of integers in tensor rows into single integers using Cantor pairing.\n",
    "    The function recursively applies the Cantor pairing function to pairs of elements\n",
    "    in the input tensor until it encodes the entire list into a single integer.\n",
    "\n",
    "    If the input tensor is empty, the function returns 0.\n",
    "\n",
    "    Args:\n",
    "        tnsr (torch.Tensor): A tensor containing a list of integers.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Cantor encoded integer representing the input list.\n",
    "    \"\"\"\n",
    "    if tnsr.size(1) == 0:\n",
    "        # Termination point. The added 0 has no effect on the returned integer\n",
    "        return torch.tensor(0)\n",
    "    else:\n",
    "        return cantor_pairing(tnsr[:, 0], cantor_encode_tensor(tnsr[:, 1:]))\n",
    "\n",
    "\n",
    "class WalksDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to handle sequences of node walks.\n",
    "\n",
    "    Args:\n",
    "        dag_data (object): The input DAG data.\n",
    "        dict_cantor_to_honode_ixs_mapping (dict): Dictionary mapping Cantor encoded indices to higher-order node indices.\n",
    "        max_order (int): Maximum order of nodes in a walk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dag_data, dict_cantor_to_honode_ixs_mapping, max_order):\n",
    "        self.max_order = max_order\n",
    "        self.dict_cantor_to_honode_ixs_mapping = dict_cantor_to_honode_ixs_mapping\n",
    "        self._preprocess_data(dag_data.dags)\n",
    "        self._create_tensors_and_encodings()\n",
    "\n",
    "    def _preprocess_data(self, dags):\n",
    "        \"\"\"\n",
    "        Preprocesses the DAG data to extract walks and their counts.\n",
    "\n",
    "        Args:\n",
    "            dags DAGData object\n",
    "        \"\"\"\n",
    "        self.walks_by_length = {}\n",
    "        self.walk_counts_by_length = {}\n",
    "        self.total_sequences = 0\n",
    "        \n",
    "        for dag in dags:\n",
    "            node_seq_path = dag.node_sequence.T[0]\n",
    "            seq_length = len(node_seq_path)\n",
    "            if seq_length not in self.walks_by_length:\n",
    "                self.walks_by_length[seq_length] = []\n",
    "                self.walk_counts_by_length[seq_length] = []\n",
    "            self.walks_by_length[seq_length].append(node_seq_path)\n",
    "            # Answer to Moritz question\n",
    "            # probably one weight per DAG would be enough\n",
    "            self.walk_counts_by_length[seq_length].append(int(dag.edge_weight.unique()))\n",
    "            self.total_sequences += 1\n",
    "\n",
    "    def _create_tensors_and_encodings(self):\n",
    "        \"\"\"\n",
    "        Creates tensors and encodings for the walks.\n",
    "        \"\"\"\n",
    "        self.walk_tensors_by_length = {\n",
    "            length: torch.stack(walks, dim=0)\n",
    "            for length, walks in self.walks_by_length.items()\n",
    "        }\n",
    "        self.bipartite_encoded_walks_by_length = {\n",
    "            length: self._bipartite_encode(length)\n",
    "            for length in self.walk_tensors_by_length\n",
    "        }\n",
    "\n",
    "    def _bipartite_encode(self, walk_length):\n",
    "        \"\"\"\n",
    "        Encodes the walks in bipartite form, i.e.,\n",
    "        representing transitions between indexes of higher-order nodes.\n",
    "\n",
    "        In bipartite encoding, each transition in the walk sequence corresponds to a pair of indexes,\n",
    "        where the first `self.max_order` transitions utilize indexes from the higher-order nodes of the i-th order tensors, and the subsequent transitions use indexes of the `self.max_order` most recent nodes.\n",
    "\n",
    "        Args:\n",
    "            walk_length (int): The length of the walk.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Bipartite-encoded walk sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        list_cantor_node_ixs_tensors = []\n",
    "        \n",
    "        for i in range(1, walk_length + 1):\n",
    "            hon_ixs_tensor = self.walk_tensors_by_length[walk_length][:, max(0, i - self.max_order):i]\n",
    "            cantor_encoded = cantor_encode_tensor(hon_ixs_tensor)\n",
    "            mapped_indices = cantor_encoded.apply_(self.dict_cantor_to_honode_ixs_mapping[min(i, self.max_order)].get)\n",
    "            list_cantor_node_ixs_tensors.append(mapped_indices)\n",
    "            \n",
    "        return torch.stack(list_cantor_node_ixs_tensors, dim=1)\n",
    "\n",
    "    def __getitem__(self, index_tuple):\n",
    "        \"\"\"\n",
    "        Retrieves a bipartite-encoded node sequence from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index_tuple (tuple): A tuple containing the length of the sequence (int) and the index of the sequence within that length (int).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The bipartite-encoded node sequence.\n",
    "        \"\"\"\n",
    "        # This cannot work with batching. Could work if each walk length had its own Dataset object\n",
    "        walk_length, index = index_tuple\n",
    "        return self.bipartite_encoded_walks_by_length[walk_length][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of sequences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of sequences.\n",
    "        \"\"\"\n",
    "        return self.total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_data = pp.DAGData(pp.IndexMap(list(\"01234\")))\n",
    "\n",
    "dag_data.append_walk(list(\"0230230\"), weight=30)\n",
    "dag_data.append_walk(list(\"1241241\"), weight=70)\n",
    "dag_data.append_walk(list(\"0230241\"), weight=1)\n",
    "#\n",
    "# dag_data.append_walk(list(\"0230230230230230230\"), weight=30)\n",
    "\n",
    "\n",
    "m = pp.MultiOrderModel.from_DAGs(dag_data, max_order=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cantor_econding_mon_nodes(m):\n",
    "    \"\"\" \n",
    "    Applies cantor encoding to the node sequences of the higher-order nodes for all the layers of a MultiOrderModel\n",
    "    Returns a dictionary containing, for each layer, a mapping from the cantor index (encoding) of the sequence to the index of the ho-node in the Graph object\n",
    "    \"\"\"\n",
    "    dict_cantor_to_honode_ixs_mapping = {}\n",
    "    # This works cause the ho-node sequences are sorted by their indices\n",
    "    for order, hon in m.layers.items():\n",
    "        cantor_ids = cantor_encode_tensor(hon.data.node_sequence)\n",
    "        cantor_to_node_ixs_mapping = {cantor_id.item(): i for i, cantor_id in enumerate(cantor_ids)}\n",
    "        dict_cantor_to_honode_ixs_mapping[order] = cantor_to_node_ixs_mapping\n",
    "    return dict_cantor_to_honode_ixs_mapping\n",
    "dict_cantor_to_honode_ixs_mapping = get_cantor_econding_mon_nodes(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_data = WalksDataset(dag_data, dict_cantor_to_honode_ixs_mapping, max_order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: following cells contraints all this to be done on a single value of path length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 7\n",
    "source_to_target_from_walks = walk_data.bipartite_encoded_walks_by_length[l]\n",
    "path_counts = walk_data.walk_counts_by_length[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND ANOTHER WAY TO GET NODE COUNTS\n",
    "from torch_geometric.loader import DataLoader\n",
    "dag_graph = next(iter(DataLoader(dag_data.dags, batch_size=len(dag_data.dags)))).to(pp.config[\"torch\"][\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-449.9671)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_nodes, counts = torch.unique(dag_graph.node_sequence, return_counts=True)\n",
    "node_emission_probabilities = counts / counts.sum()\n",
    "\n",
    "source_to_target_edge_index_zeroth = torch.stack([\n",
    "    torch.zeros_like(source_to_target_from_walks[:, 0]),\n",
    "    source_to_target_from_walks[:, 0]\n",
    "])\n",
    "\n",
    "# log likelihood\n",
    "tot_log_lh = 0\n",
    "# log likelihood for the 0-th steps\n",
    "lh_l = torch.mul(torch.log(node_emission_probabilities[source_to_target_edge_index_zeroth[1]]), torch.tensor(path_counts))\n",
    "tot_log_lh += lh_l.sum()\n",
    "\n",
    "for i in range(0, l - 1):\n",
    "    T = compute_transition_probabilities(m.layers[min(i + 1, walk_data.max_order)])\n",
    "    # Prepare source_to_target_edge_index\n",
    "    source_to_target_edge_index = source_to_target_from_walks[:, i:i + 2].T.squeeze()\n",
    "    # log likelihood for i-th steps\n",
    "    lh_l = torch.mul(torch.log(T[source_to_target_edge_index[1]]), torch.tensor(path_counts))\n",
    "    tot_log_lh += lh_l.sum()\n",
    "\n",
    "tot_log_lh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adapt to multiple walk lengths\n",
    "- Better structure for package\n",
    "- Degrees of freedom etc (actually making the likilhood ration test test)\n",
    "    - This all thing above is getting lh (organize)\n",
    "    - Need degrees of freedom\n",
    "    - Need to actually perform che likelihood ratio test (see line 407 of this: https://github.com/IngoScholtes/pathpy/blob/master/pathpy/MultiOrderModel.py#L378). \n",
    "- test functions \n",
    "    - Dof computation\n",
    "    - lh computation\n",
    "    - encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-449.9671)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mon_likelihood(mon, walk_data):\n",
    "\n",
    "    dict_cantor_to_honode_ixs = get_cantor_econding_mon_nodes(mon)\n",
    "    inner_walk_data = WalksDataset(walk_data, dict_cantor_to_honode_ixs, max_order=max(mon.layers))\n",
    "\n",
    "    # zeroth order part:\n",
    "    # TODO: FIND ANOTHER WAY TO GET NODE COUNTS for node emission probabilities ###########\n",
    "    # notice that this is building the zeroth order of a MultiOrderModel\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    dag_graph = next(iter(DataLoader(dag_data.dags, batch_size=len(walk_data.dags)))).to(pp.config[\"torch\"][\"device\"])\n",
    "    __unique_nodes, counts = torch.unique(dag_graph.node_sequence, return_counts=True)\n",
    "    node_emission_probabilities = counts / counts.sum()\n",
    "    #####################\n",
    "    # at the zeroth order, source_to_target are just connections from start node to any of the first order nodes\n",
    "    source_to_target_edge_index_zeroth = torch.stack([\n",
    "        torch.zeros_like(source_to_target_from_walks[:, 0]),\n",
    "        source_to_target_from_walks[:, 0]\n",
    "    ])\n",
    "\n",
    "    # Beginnign of log likelihood computation\n",
    "    tot_log_lh = 0\n",
    "    for walk_length in inner_walk_data.walks_by_length:\n",
    "        # log likelihood for the 0-th order\n",
    "        path_counts = inner_walk_data.walk_counts_by_length[walk_length]\n",
    "        lh_l = torch.mul(torch.log(node_emission_probabilities[source_to_target_edge_index_zeroth[1]]), torch.tensor(path_counts))\n",
    "        tot_log_lh += lh_l.sum()\n",
    "\n",
    "        # TODO: modify name 'source_to_target'. These are paths represented with the indexes of mon nodes \n",
    "        source_to_target = inner_walk_data.bipartite_encoded_walks_by_length[l]\n",
    "        for i in range(0, l - 1):\n",
    "            T = compute_transition_probabilities(m.layers[min(i + 1, inner_walk_data.max_order)])\n",
    "            # Prepare source_to_target_edge_index\n",
    "            source_to_target_edge_index = source_to_target[:, i:i + 2].T.squeeze()\n",
    "            # log likelihood for i-th steps\n",
    "            lh_l = torch.mul(torch.log(T[source_to_target_edge_index[1]]), torch.tensor(path_counts))\n",
    "            tot_log_lh += lh_l.sum()\n",
    "    return tot_log_lh\n",
    "\n",
    "get_mon_likelihood(m, dag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_mon_dof(m,max_order, assumption = \"paths\"):\n",
    "    \"\"\"\n",
    "    The degrees of freedom fo the kth layer of a multi-order model this depende on the number of different paths of exactly length k in the graph.\n",
    "    Therefore, we can obtain this values by summing the entries of the kth power of the binary adhacency matrix of the graph.\n",
    "    Finally, we must consider that, due the conservation of probablility, all non-zero rows of the transition matrix of the higher-order network must sum to one. \n",
    "    This poses on additional constraint per row that respects the condition, which should be removed from the total count of degrees of freedom.\n",
    "\n",
    "    @param: maxOrder: the maximum order up to which model layers shall be\n",
    "    taken into account\n",
    "\n",
    "    @param assumption: if set to 'paths', for the degree of freedom calculation\n",
    "        only paths in the first-order network topology will be considered. This is\n",
    "        needed whenever we model paths in a *given* network topology.\n",
    "        If set to 'ngrams' all possible n-grams will be considered, independent of whether they\n",
    "        are valid paths in the first-order network or not. The 'ngrams' and the 'paths' assumption\n",
    "        coincide if the first-order network is fully connected, i.e. if all possible paths actually occur.\n",
    "    \"\"\"\n",
    "    if max_order == None:\n",
    "        max_order = max(m.layers)\n",
    "    assert max_order <= max(m.layers), 'Error: maxOrder cannot be larger than maximum order of multi-order network'\n",
    "\n",
    "    # for both assumptions, the zeroth order simply gives the node probabilities. \n",
    "    dof = m.layers[1].data.num_nodes - 1\n",
    "\n",
    "    if assumption == \"paths\":\n",
    "        for order in range(1, max_order+1):\n",
    "            num_order_paths = m.layers[order].data.num_nodes\n",
    "            num_nonzero_outdegrees = len(m.layers[order].data.edge_index[0].unique())\n",
    "            dof += num_order_paths - num_nonzero_outdegrees\n",
    "    elif assumption == \"ngram\":\n",
    "        # the ngram assumption corresponds to the assumption normally made when using higher-order markov chains\n",
    "        # i.e., every node can follow every predecent sequence of nodes\n",
    "        for order in range(1, max_order + 1):\n",
    "            dof += (m.layers[1].data.num_nodes**order)*(m.layers[1].data.num_nodes - 1) \n",
    "    else:\n",
    "        assert False, f\"Unknown assumption {assumption} in input. The only accepted values are 'path' and 'ngram' \"\n",
    "    \n",
    "    return int(dof)\n",
    "\n",
    "get_mon_dof(m,max_order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.layers[1].data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:48\u001b[0;36m\u001b[0m\n\u001b[0;31m    def estimateOrder(self, paths, maxOrder=None, significanceThreshold=0.01):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "def likeliHoodRatioTest(self, paths, maxOrderNull=0, maxOrder=1, assumption='paths', significanceThreshold=0.01):\n",
    "        \"\"\"\n",
    "        Performs a likelihood-ratio test between two multi-order models with given maximum orders, where maxOrderNull serves\n",
    "        as null hypothesis and maxOrder serves as alternative hypothesis. The null hypothesis is rejected if the p-value for\n",
    "        the observed paths under the null hypothesis is smaller than the given significance threshold.\n",
    "\n",
    "        Applying this test makes the assumption that we have nested models, i.e. that the null model is contained\n",
    "        as a special case in the parameter space of the more complex model. If we assume that the path constraint holds,\n",
    "        this is not true for the test of the first- against the zero-order model (since some sequences of the zero order model\n",
    "        cannot be generated in the first-order model). However, since the set of possible higher-order transitions is generated\n",
    "        based on the first-order model, the nestedness property holds for all higher order models.\n",
    "\n",
    "        @param paths: the path data to be used in the liklihood ratio test\n",
    "        @param maxOrderNull: maximum order of the multi-order model\n",
    "                to be used as a null hypothesis\n",
    "        @param maxOrder: maximum order of the multi-order model to be used as\n",
    "                alternative hypothesis\n",
    "        @param assumption: paths or ngrams\n",
    "        @param significanceThreshold: the threshold for the p-value\n",
    "                below which to accept the alternative hypothesis\n",
    "        @returns: a tuple of the format (reject, p) which captures whether or\n",
    "                not the null hypothesis is rejected in favor of the alternative\n",
    "                hypothesis, as well as the p-value that led to the decision\n",
    "        \"\"\"\n",
    "\n",
    "        assert maxOrderNull < maxOrder, 'Error: order of null hypothesis must be smaller than order of alternative hypothesis'\n",
    "        # let L0 be the likelihood for the null model and L1 be the likelihood for the alternative model\n",
    "\n",
    "        # we first compute a test statistic x = -2 * log (L0/L1) = -2 * (log L0 - log L1)\n",
    "        x = -2 * (self.getLikelihood(paths, maxOrder=maxOrderNull, log=True) - self.getLikelihood(paths, maxOrder=maxOrder, log=True))\n",
    "\n",
    "        # we calculate the additional degrees of freedom in the alternative model\n",
    "        dof_diff = self.getDegreesOfFreedom(maxOrder=maxOrder, assumption=assumption) - self.getDegreesOfFreedom(maxOrder=maxOrderNull, assumption=assumption)\n",
    "\n",
    "        Log.add('Likelihood ratio test for K_opt = ' + str(maxOrder) + ', x = ' + str(x))\n",
    "        Log.add('Likelihood ratio test, d_1-d_0 = ' + str(dof_diff))\n",
    "\n",
    "        # if the p-value is *below* the significance threshold, we reject the null hypothesis\n",
    "        p = 1-chi2.cdf(x, dof_diff)\n",
    "\n",
    "        Log.add('Likelihood ratio test, p = ' + str(p))\n",
    "        return (p<significanceThreshold), p\n",
    "\n",
    "\n",
    "    def estimateOrder(self, paths, maxOrder=None, significanceThreshold=0.01):\n",
    "        \"\"\"\n",
    "        Selects the optimal maximum order of a multi-order network model for the\n",
    "        observed paths, based on a likelihood ratio test with p-value threshold of p\n",
    "        By default, all orders up to the maximum order of the multi-order model will be tested.\n",
    "\n",
    "        @param paths: The path statistics for which to perform the order selection\n",
    "\n",
    "        @param maxOrder: The maximum order up to which the multi-order model shall be tested.\n",
    "        \"\"\"\n",
    "        if maxOrder == None:\n",
    "            maxOrder = self.maxOrder\n",
    "        assert maxOrder <= self.maxOrder, 'Error: maxOrder cannot be larger than maximum order of multi-order network'\n",
    "        assert maxOrder > 1, 'Error: maxOrder must be larger than one'\n",
    "\n",
    "        maxAcceptedOrder = 1\n",
    "\n",
    "        # Test for highest order that passes\n",
    "        # likelihood ratio test against null model\n",
    "        for k in range(2, maxOrder+1):\n",
    "            if self.likeliHoodRatioTest(paths, maxOrderNull=k-1, maxOrder=k, significanceThreshold=significanceThreshold)[0]:\n",
    "                maxAcceptedOrder = k\n",
    "\n",
    "        return maxAcceptedOrder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
